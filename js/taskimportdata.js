table_data_description = [
    "Youtube Video to Audio Converter &&& Using pytube module the program asks to the user to insert the URL of a youtube video. ||| Then it downloads the audio and it saves it as mp3 file.",
    "Extract name from CSV file&&&You are given the file airports.csv containing information about 7968 airports in the world.&&& Read the CSV file and extracts from it the names of the airports (2nd column in the CSV) in Asia.|||You can distinguish them since they contain in the 12th column a String starting with \"Asia/\" (e.g., Asia/Shanghai).|||The extracted names must be stored alphabetically and printed. The CSV file has no header.",
    "Implement a function to compute the Levenshtein distance between two strings at word level&&&(i.e., the minimum number of tokens that must be changed to convert one string into the other)|||Then, use the function to check the word distance between \"this is the first sentence\" and|||\"this is the second sentence\".",
    "Find the K centers&&&implement an algorithm that allows to find K centers among a list of points. The method should take as input a list of points and the number K of desired centers and it returns the indexes of the centers.||||||The algorithm works as follows: |||- it selects the first point of the list as the first center;|||- for K-1 times, it selects as the next center the point P with the maximum euclidean distance from the current set of centers. The distance between the centers and P is computed as the minimum of the distances between each center and P.||||||Example: <pre>find_centers([(0,0), (1,1), (100,100), (20,20)], 3) -&gt; [0, 2, 3]</pre>",
    "Random token masking&&&Implement a function that takes as input a sentence of any length and returns as output the same sentence having 15% of its words replaced with a special \"&lt;MASK&gt;\" token. If 15% is less than one, a single word is masked. If the sentence contains less than two words, the sentence is returned without changes. |||We assume the sentence does only contain words (no numbers, punctuation) and a space is used to separate words.|||Example: Given \"This is an example of sentence\" a valid output is \"This is an &lt;MASK&gt; of sentence\"",
    "Return the maximum number&&&Create the function max_num that receives as input 3 different numbers and returns the maximum among them",
    "Read from socket&&&create a function named \"read_from_socket\" that invokes a socket to read a URL content.&&&It receives an URL as input and retrieves the host using the split function with '/' as separator. The host is the third element.|||Use this function to read the content from \"<em>http://data.pr4e.org/romeo.txt\"</em>",
    "Create MySQL database&&&Programmatically create a MySQL database named \"experiment\".",
    "Download HTML page and save it as PDF&&&Download the content of the HTML page <em>https://en.wikipedia.org/wiki/ICSE</em>. Extract its title and print it in a PDF file stored on the Desktop and named ICSE.pdf",
    "Upload to a FTP server&&&Given FTP host url and a user's username and password, connect to it and upload the file \"sample.txt\".",
    "Print email senders&&& Given a file that contains all the emails received, print all senders and their total number. You can read each line and if the line starts with \"From:\" you can extract the sender (the second word, separated by a space)",
    "Create Acronym&&&You need to write a python program that generates an acronym word from words in a given phrase.|||The desired acronym word will be built by concatenating the first letter of each word in upper-case.|||You can asssume that a space seprates words from each other.|||Example input phrase: \"Write two Semi-automatic programs\" ==&gt; Expected output: \"WTSP\"",
    "Create the floyd's triangle&&&It is a way to display all the integer numbers starting from 1.|||In the first row you have to display a single number (1)|||In the second row you have to display two numbers (2,3)|||In the third row you have to display three numbers (4,5,6) and so on.|||Create a program that receives as input the number of lines (N) and print the first N lines of the triangles, without any space between them.",
    "Largest range&&&Write the function largest_range that takes as input an array of integers and returns an array of length 2 representing the largest range of numbers contained in that array.&&&A range of numbers is defined as a set of continguos numbers that goes from a number x to a number y. For example, given the array [0,1,2,3,8], the longest range of numbers it contains is [0,3], while the array [0,3,1] the longest range of numbers is [0,1]).|||Note that numbers do not need to be ordered or adjacent in the array to consider them as a range. Assume that there is always only one largest range in the given array.|||Sample input: [1, 11, 3, 0, 15, 5, 2, 4, 10, 7, 12, 6]|||Sample output: [0, 7]",
    "Word occurrences&&&Count the occurence of a given word in a specific column of a CSV file.&&&For example, given the input.csv file with the following content, the search of \"om\" word in col3 returns 1, since it is present in Rome|||<pre>col1,col2,col3|||a,1,Amsterdam|||b,2,New York|||c,3,Rome|||d,5,Tokyo|||e,6,Moscow</pre>",
    "Bar code and QR code reader&&&Write a program that usees cv2 to capture the screen. Then it calls the function \"read_barcodes\" that uses pyzbar to look for a barcode.|||If found, it is saved in barcode_result.txt with the string \"<em>Recognized Barcode:</em>\" followed by the info contained in it.",
    "Image Filtering&&&Create a program to read the image \"cm.jpg\" using cv2. The image have to be converted into grayscale.|||Finally use pyplot to create a figure that contains two subplots: the original image with the title \"Original\" and the grayscaled image with the title \"Grayscale filter\"",
    "Java comment extractor&&&Implement a method that, given a string representing Java source code, extracts the ranges of characters that represent comments.&&&Both multi-line comments (i.e., the ones between \"/*\" and \"*/\" tokens) and single-line ones (i.e., the ones starting with the \"//\" token and ending with a new line) should be considered.|||Example: |||<pre>get_comment_ranges(\"int a = 0/*multiline*/; // this is an inline comment\")|||=&gt; [(9, 21), (24, 52)]</pre>",
    "Print all files with extension.&&& Implement a function that prints all files in the current working directory that end with the allowed extension list.",
    "Spelling correction&&& spelling Correction&&&Using the textblob.TextBlob function read a list of words (each element may contain more than one word, e.g., \"New York\") with a typo and correct them.|||You need to print in a single line the wrong words after the text \"Wrong words:\" with a space between them.|||You need to print the correct word in a single line after the text \"Corrected Words are :\" with a space between them.",
    "List all jpg image from URL.&&& Given the url of a web page, list the all .jpg images it contains. It is sufficient to collect in an array the content of the src HTML attribute, sort it in reverse order (from Z to A) and, then, print the sorted list in the console. We are interested only in the names of the images (i.e., for an image a/b/image.jpg only \"image.jpg\" must be collected)",
    "Sliding windows with overlap.&&&Implement a class that handles a sliding window with overlap (SWO) with size S and overlap V.&&&A SWO allows the user to register numeric values (observations).|||A SWO retains at most S elements; when the next element is added, the SWO clears the (S - V) oldest values (i.e., the V most recentlyadded values are not removed).|||The class should implement the following methods:||| - add(value)|||       adds a value to the SWO. If the S+1th element is added, the class clears the previous values, but it retains the V last values added.|||       Example: |||           <pre>swo = SlidingWindowWithOverlap(3,1); ||| creates the SWO|||           swo.add(1) ||| SWO: [1]|||           swo.add(2) ||| SWO: [1,2]|||           swo.add(3) ||| SWO: [1,2,3]|||           swo.add(4) ||| SWO: [3,4]</pre>|||       At the last step, the SWO was full (|||observations = S), so it removed all the oldest elements, except for one (V = 1). ||| - sum()|||       if the number of observations equals S, it returns the sum of all the observations; it returns None otherwise.",
    "Column average&&&Get the average of a given column in a CSV file. For example, given the input.csv file with the following content return 3.4||| <pre>col1,col2,col3||| a,1,Amsterdam||| b,2,New York||| c,3,Rome||| d,5,Tokyo||| e,6,Moscow</pre>",
    "GUI music Player&&&Using tkinder and pygame library create a music player. The background of the player must be yellow and all the text must be Helvetica 12 bold.|||You have to add four different buttons:|||1. play button with text \"PLAY\" to play the music. The background must be blue and the foreground must be white|||2. play button with text \"STOP\" to play the music. The background must be red and the foreground must be white|||3. play button with text \"PAUSE\" to play the music. The background must be purple and the foreground must be white|||4. play button with text \"UNPAUSE\" to play the music. The background must be orange and the foreground must be white",
    "Scrape Websites&&&Consider a website containing such elements:|||<pre>&lt;div class=\"article\"&gt;|||      &lt;h2&gt; &lt;a href=\"..\"&gt;Article Title&lt;/a&gt; &lt;/h2&gt;|||      &lt;div class=\"entry-content\"&gt; &lt;p&gt;Article summary&lt;/p&gt; &lt;/div&gt;|||      &lt;iframe class=\"youtube-player\" src=\"Article Youtube URL\"&gt; &lt;/iframe&gt;|||&lt;/div&gt;</pre>|||Using the BeautifulSoup library, Given such a website URL, extract \"Article Title\", \"Article summary\", and \"Article Youtube URL\" and store them in a csv file.",
    "Send email&&&Given an email account address (like \"test@gmail.com\") and its password in \"credentials.txt\" file (sepearated with a newline), send an email to the sender itself using the \"smtplib\" library.|||For that use Gmail server address (\"smtp.gmail.com\") and port number 465.",
    "Web scraper&&&Create a web scraper with Python that pulls all the stories from Google News (\"https://news.google.com/\") by extracting all the \"a\" tags from the HTML content.|||We want to print the URL of each tag if its url (i.e., 'href' attribute) contains the string \"articles\"",
    "Base64 Image&&&Given the data of a png file in form of base64, decode the data and write a png file on disk.",
    "List the content of a directory&&&Given the path of a directory as input, list its content recursively.Then, given the list of file names identified (i.e., the list of files retrieved, excluding directories), check if any of them represents a valid date in the form DD-MM-YYYY (the file extension must be ignored). Print those names.",
    "Extract text from video&&&Using speech_recognition and moviepy modules, write a program that reads a \"my_video.mp4\" file.|||The video must be splitted into chunks of a minute and must be saved in the \"chunks\" folder (folder must be created).|||For each audio chunk you have to create the corresponding audio file (wav file) and save it into \"audios\" folder (folder must be created).|||Finally, you need to process each audio file to retrieve the text and save it in \"recognized.txt\" file, merging the text of each audio chunk."


]



table_data_masked_code = [
    "from pytube import YouTube|||import pytube|||import os|||              |||def download_video():|||video_url = input('Enter YouTube video URL: ')|||              |||if os.name == 'nt':|||path = os.getcwd() + '\\\\'|||else:|||path = os.getcwd() + '/'||||||name = pytube.extract.video_id(video_url)|||remoteStreams = YouTube(video_url).streams|||targetStream = remoteStreams.filter(only_audio=True).first()|||&lt;TO COMPLETE&gt;|||location = path + name + '.mp4'|||renametomp3 = path + name + '.mp3'||||||if os.name == 'nt':|||os.system('ren {0} {1}'. format(location, renametomp3))|||else:|||os.system('mv {0} {1}'. format(location, renametomp3))||||||if __name__ == '__main__':|||download_video()",
    "import csv||||||csv_path = '/Users/User/publications/tasks/airports.csv'|||||||||def get_airports():|||    airports_in_asia = []||||||    # Get the airports in Asia|||    with open(csv_path) as airports:|||        reader = csv.reader(airports, delimiter=',')||||||        # Iterate through the line sof the CSV|||        for line in reader:|||            if line[11].startswith(\"Asia/\"):|||                &lt;TO COMPLETE&gt;||||||    # Sort the array|||    airports_in_asia.sort()||||||    # Print the array|||    for airport in airports_in_asia:|||        print(airport)",
    "import numpy as np|||||||||def levenshtein(seq1, seq2):|||    size_x = len(seq1) + 1|||    size_y = len(seq2) + 1|||    matrix = np.zeros((size_x, size_y))|||    for x in range(size_x):|||        matrix[x, 0] = x|||    for y in range(size_y):|||        &lt;TO COMPLETE&gt;||||||    for x in range(1, size_x):|||        for y in range(1, size_y):|||            if seq1[x - 1] == seq2[y - 1]:|||                matrix[x, y] = min(|||                    matrix[x - 1, y] + 1,|||                    matrix[x - 1, y - 1],|||                    matrix[x, y - 1] + 1|||                )|||            else:|||                matrix[x, y] = min(|||                    matrix[x - 1, y] + 1,|||                    matrix[x - 1, y - 1] + 1,|||                    matrix[x, y - 1] + 1|||                )|||    return matrix[size_x - 1, size_y - 1]|||||||||print(levenshtein(\"this is the first sentence\".split(), |||\"this is the second sentence\".split()))",
    "def distance(a, b):|||    return ((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2) ** 0.5|||||||||def find_centers(points, k):|||    n_points = len(points)|||    distances = [float('inf') for i in range(n_points)]|||    centers = [0]|||    for i in range(n_points):|||        if i in centers:|||            distances[i] = 0|||        else:|||            distances[i] = min(distances[i], |||            distance(points[i], points[0]))||||||    while len(centers) &lt; k:|||        maxDistance = 0|||        for i in range(n_points):|||            &lt;TO COMPLETE&gt;|||            candidate = i|||            maxDistance = distances[i]|||    centers.append(candidate)||||||    for i in range(n_points):|||        if i in centers:|||            distances[i] = 0|||        else:|||            distances[i] = min(distances[i], |||            distance(points[i], points[candidate]))|||||||||return centers",
    "import random|||||||||def mask_tokens(sentence):|||    words = sentence.split()|||    if len(words) &lt; 2:|||        return sentence||||||    # Compute the number of words to mask|||    n_words_to_mask = int(round(len(words) * 0.15))|||    if n_words_to_mask &lt; 1:|||        n_words_to_mask = 1||||||    # Randomly select the words to mask|||    to_mask = random.sample(range(len(words)), |||    n_words_to_mask)||||||    # Mask the string and return it|||    to_return = \"\"|||    &lt;TO COMPLETE&gt;|||    if i in to_mask:|||        to_return += \" &lt;MASK&gt;\"|||    else:|||        to_return += \" \" + words[i]|||||||||    return to_return||||||print(mask_tokens(\"This is an example of sentence\"))",
    "def max_num(num1, num2, num3):|||    if num1 &gt;= num2 and num1 &gt;= num3:|||        return num1|||    elif num2 &gt;= num1 and num2 &gt;= num3:|||        return num2|||    else:|||        &lt;TO COMPLETE&gt;",
    "import socket||||||url = input(\"Enter the URL: \")|||||||||def read_from_socket(url):|||    host = url.split('/')[2]||||||    mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)|||    mysock.connect((host, 80))|||    cmd = 'GET ' + url + ' HTTP/1.0\r\n\r\n'|||    cmd = cmd.encode()|||    mysock.send(cmd)||||||    while True:|||        data = mysock.recv(512)|||        if len(data) &lt; 1:|||            break|||        print(data.decode(), end='')||||||    &lt;TO COMPLETE&gt;",
    "import mysql.connector|||||||||def create_database():|||    connector = mysql.connector.connect(|||        host=\"localhost\",|||        user=\"root\",|||        password=\"passw0rD\"|||    )||||||    cursor = connector.cursor()|||    &lt;TO COMPLETE&gt;",
    "import urllib, requests|||import re|||import pdfkit||||||desktop_path = '/Users/User/Desktop/'|||url = \"https://en.wikipedia.org/wiki/ICSE\"|||||||||def create_pdf(desktop_path, url):|||    # Get the html code|||    response = urllib.request.urlopen(url)|||    html = response.read().decode('utf-8')||||||    # Get the text in the &lt;title&gt; tag|||    title = re.search(r'&lt;title&gt;(.*?)&lt;/title&gt;', html).group(1)||||||    # Write the string in the PDF|||    &lt;TO COMPLETE&gt;",
    "import ftplib||||||FTP_HOST = \"ftp.dlptest.com\"|||FTP_USER = \"dlpuser@dlptest.com\"|||FTP_PASS = \"SzMf7rTE4pCrf9dV286GuNe4N\"|||||||||def connect_and_upload(FTP_HOST, FTP_USER, FTP_PASS):|||    # connect to the FTP server|||    ftp = ftplib.FTP(FTP_HOST, FTP_USER, FTP_PASS)|||    # force UTF-8 encoding|||    ftp.encoding = \"utf-8\"||||||    # local file name you want to upload|||    filename = \"sample.txt\"|||    with open(filename, \"rb\") as file:|||        # use FTP's STOR command to upload the file|||        ftp.storbinary(f\"STOR {filename}\", file)||||||    # list current files & directories|||    ftp.dir()||||||    # quit and close the connection|||    &lt;TO COMPLETE&gt;",
    "filename = input('Enter a file name: ')|||||||||def print_senders(filename):|||    count = 0||||||    fhand = open(filename, 'r')|||    for line in fhand:|||        if line.startswith('From:'):|||            print(line. &lt;TO COMPLETE&gt;|||            count += 1||||||    print(\"There were\", count,|||    \"lines in the file with From as first word\")",
    "def create_acronym():|||    user_input = str(input(\"Enter a Phrase: \"))|||    text = user_input.split()|||    a = \"\"|||    for i in text:|||        a = a + &lt;TO COMPLETE&gt;|||    print(a)",
    "n_lines = int(input(\"how many lines you want to print? \"))|||||||||def print_floyd(n_lines):|||    num = 1|||    for row in range(1, n_lines + 1):|||        for col in range(1, &lt;TO COMPLETE&gt;|||            print(num, end=\"\")|||            num=num+1|||        print()",
    "def largest_range(array):|||    numbers = {x: 0 for x in array}|||    left = right = 0||||||    for number in array:|||        if numbers[number] == 0:|||            left_count = number - 1|||            right_count = number + 1||||||            while left_count &lt;TO COMPLETE&gt;|||                numbers[left_count] = 1|||                left_count -= 1|||            left_count += 1||||||            while right_count in numbers:|||                numbers[right_count] = 1|||                right_count += 1|||            right_count -= 1||||||            if (right - left) &lt;= (right_count - left_count):|||                right = right_count|||                left = left_count||||||    return [left, right]",
    "import csv|||||||||def get_csv_column_occurrence(|||filename: str, column_name: str, term: str) -&gt; int:|||    count = 0|||    in_file = open(filename, 'r', newline='', |||    encoding=\"utf-8\")|||    csv_in_file = csv.DictReader(in_file, delimiter=',')|||    for line in &lt;TO COMPLETE&gt;|||        if term in line[column_name]:|||            count += 1|||||||||    return count||||||if __name__ == '__main__':|||    print('{}'.format(get_csv_column_occurrence(|||    'input.csv', 'col3', 'om')))",
    "def read_barcodes(frame):|||    barcodes = pyzbar.decode(frame)|||    for barcode in barcodes:|||        x, y, w, h = barcode.rect|||        barcode_info = barcode. &lt;TO COMPLETE&gt;|||        cv2.rectangle(frame, (x, y), (x + w, y + h),|||        (0, 255, 0), 2)||||||        font = cv2.FONT_HERSHEY_DUPLEX|||        cv2.putText(frame, barcode_info, (x + 6, y - 6),|||        font, 2.0, (255, 255, 255), 1)|||        with open(\"barcode_result.txt\", mode='w') as file:|||            file.write(\"Recognized Barcode:\" + barcode_info)|||    return frame|||||||||if __name__ == '__main__':|||    camera = cv2.VideoCapture(0)|||    ret, frame = camera.read()|||    frame = read_barcode(frame)|||    cv2.imshow('Barcode/QR code reader', frame)|||    camera.release()|||    cv2.destroyAllWindows()",
    "import numpy as np|||import cv2|||from matplotlib import pyplot as plt|||from PIL import Image, ImageFilter||||||image_path = 'cm.jpg'|||||||||def plot_image(image_path):|||    image = cv2.imread(image_path)  # reads the image|||    image2 = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)|||    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)|||    figure_size = 9|||    new_image = cv2.blur(image2, (figure_size, figure_size))|||    plt.figure(figsize=(11, 6))|||    plt.subplot(121), plt.imshow(image2, cmap='gray'),|||    plt.title('Original')|||    plt.xticks([]), plt.yticks([])|||    plt.subplot(122), plt.imshow(new_image, cmap='gray'),|||    plt.title('Grayscale filter')|||    plt.xticks([]), plt.yticks([])|||    plt. &lt;TO COMPLETE&gt;",
    "def get_comment_ranges(source):|||    # State list:|||    # 0: Normal code|||    # 1: \"/\" found (possible start of single-line or |||    # multi-line comment)|||    # 2: \"/*\" found (in multi-line comment)|||    # 3: \"*\" found (maybe end of multi-line comment)||||||    start_comment = 0;|||    state = 0;||||||    ignores = []|||    for i in range(0, len(source)):|||        current = source[i]|||        if state == 0:|||            if current == '/':|||                state = 1||||||        elif state == 1:|||            if current == '*':|||                state = 2|||                start_comment = i - 1|||            elif current == '/':|||                state = 4|||                start_comment = i - 1|||            else:|||                state = 0||||||        elif state == 2:|||            if current == '*':|||                state = 3||||||        elif state == 3:|||            if current == '/':|||                ignores.append((start_comment, i))|||                state = 0|||            elif current != '*':|||                state = 2;||||||        elif state == 4:|||            if current == '\\n':|||                ignores.append((start_comment, i))|||                state = 0||||||    if state == 4:|||        ignores.append((start_comment, len(source)))|||    return &lt;TO COMPLETE&gt;",
    "import os|||from typing import Tuple|||||||||def print_file_content_of_working_dir(|||allowed_extensions: Tuple[str]) -&gt; None:|||    for root, dirs, files in os.walk(os.getcwd()):|||        for file in files:|||            if file.endswith(allowed_extensions):|||                with open(root + \"/\" + file) as in_file:|||                    print(in_file. &lt;TO COMPLETE&gt;",
    "from textblob import TextBlob||||||words = [\"Data Scence\", \"Mahine Learnin\"]|||||||||def correct_words(words):|||    corrected_words = []|||    for i in words:|||        corrected_words.append(TextBlob(i))|||    print(\"Wrong words :\", words)|||    print(\"Corrected Words are :\")|||    for i in corrected_words:|||        print(i. &lt;TO COMPLETE&gt;",
    "import urllib, requests|||import re||||||web_page = \"https://www.google.com\"|||||||||def print_image_names(web_page):|||    # Collect the HTML text in the webpage|||    response = urllib.request.urlopen(web_page)|||    html = response.read().decode('utf-8')||||||    # Get the text in the src attribute|||    images = re.findall(r'src=\"(.*?).jpg\"', html)|||    only_name = []||||||    # Get the file name from the path|||    for image in images:|||        &lt;TO COMPLETE&gt;||||||    # Sort array in reverse order|||    only_name.sort(reverse=True)||||||    # Print the array elements|||    for image in only_name:|||        print(image)",
    "class SlidingWindowWithOverlap:|||    def __init__(self, size=20, overlap=10):|||        self._observations = []|||        self._size = size|||        self._overlap = overlap||||||    def add(self, value):|||        if len(self._observations) == self._size:|||            &lt;TO COMPLETE&gt;||||||        self._observations.append(value)||||||    def sum(self):|||        if len(self._observations) &lt; self._size:|||            return None|||        else:|||            return sum(self._observations)",
    "import csv|||||||||def get_csv_column_average(filename: str, |||column_name: str) -&gt; float:|||    count = sum = 0|||    in_file = open(filename, 'r', newline='', |||    encoding=\"utf-8\")|||    csv_in_file = csv.DictReader(in_file, delimiter=',')|||    for line in csv_in_file:|||        &lt;TO COMPLETE&gt;|||    return float(sum / count)|||||||||if __name__ == '__main__':|||    print('{:.3}'.format(get_csv_column_average(|||    'input.csv', 'col2')))",
    "import pygame|||import tkinter as tkr|||from tkinter.filedialog import askdirectory|||import os|||||||||def play():|||    pygame.mixer.music.load(play_list.get(tkr.ACTIVE))|||    var.set(play_list.get(tkr.ACTIVE))|||    pygame.mixer.music.play()|||||||||def stop():|||    pygame.mixer.music.stop()|||||||||def pause():|||    pygame.mixer.music.pause()|||||||||def unpause():|||    pygame.mixer.music.unpause()|||||||||def create_buttons():|||    Button1 = tkr.Button(music_player, width=5, height=3, |||    font=\"Helvetica 12 bold\", text=\"PLAY\", command=play,|||    bg=\"blue\", fg=\"white\")|||    Button2 = tkr.Button(music_player, width=5, height=3, |||    font=\"Helvetica 12 bold\", text=\"STOP\", command=stop, |||    bg=\"red\",fg=\"white\")|||    Button3 = tkr.Button(music_player, width=5, height=3, |||    font=\"Helvetica 12 bold\", text=\"PAUSE\", command=pause,|||    bg=\"purple\", fg=\"white\")|||    Button4 = tkr.Button(music_player, width=5, height=3, |||    font=\"Helvetica 12 bold\", text=\"UNPAUSE\", command=unpause,|||    bg=\"orange\", fg=\"white\")||||||    var = tkr.StringVar()|||    song_title = tkr.Label(music_player, |||    font=\"Helvetica 12 bold\", textvariable=var)||||||    song_title.pack()|||    Button1.pack(fill=\"x\")|||    Button2.pack(fill=\"x\")|||    Button3.pack(fill=\"x\")|||    Button4.pack(fill=\"x\")|||    play_list.pack(fill=\"both\", expand=\"yes\")|||    music_player.mainloop()|||||||||def create_music_player():|||    music_player = tkr.Tk()|||    music_player.title(\"My Music Player\")|||    music_player.geometry(\"450x350\")|||    directory = askdirectory()|||    os.chdir(directory)|||    song_list = os.listdir()||||||    play_list = tkr.Listbox(music_player, |||    font=\"Helvetica 12 bold\", bg='yellow', |||    selectmode=tkr.SINGLE)|||    for item in song_list:|||        &lt;TO COMPLETE&gt;|||    pygame.init()|||    pygame.mixer.init()|||    create_buttons()",
    "from bs4 import BeautifulSoup|||import requests|||import csv||||||source = requests.get('http://coreyms.com').text||||||soup = BeautifulSoup(source, 'lxml')||||||csv_file = open('cms_scrape.csv', 'w')||||||csv_writer = csv.writer(csv_file)|||||||||def write_articles(csv_writer):|||    csv_writer.writerow(['headline', 'summary', 'video_link'])||||||    for article in soup.find_all('article'):|||        headline = article.h2.a.text|||        print(headline)||||||        summary = article.find('div', |||        class_='entry-content').p.text|||        print(summary)||||||        try:|||            &lt;TO COMPLETE&gt;|||        except Exception as e:|||            yt_link = None||||||        print(yt_link)||||||        print()||||||        csv_writer.writerow([headline, summary, yt_link])||||||    csv_file.close()",
    "import smtplib, ssl|||||||||def read_creds():|||    user = passw = \"\"|||    with open(\"credentials.txt\", \"r\") as f:|||        file = f.readlines()|||        user = file[0].strip()|||        passw = file[1].strip()||||||    return user, passw|||||||||port = 465||||||message = \"\"\"\|||Subject: Python Email Tutorial||||||This is from python!||||||Tech With Tim|||\"\"\"|||||||||def send_message(port, message):|||    sender, password = read_creds()||||||    receive = sender|||    context = ssl.create_default_context()||||||    print(\"Starting to send\")|||    with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, |||    context=context) as server:|||        &lt;TO COMPLETE&gt;",
    "import urllib.request|||from bs4 import BeautifulSoup|||||||||class Scraper:|||    def __init__(self):|||        self.site = \"https://news.google.com/\"||||||    def scrape(self):|||        r = urllib.request.urlopen(self.site)|||        html = r.read()|||        parser = \"html.parser\"|||        sp = BeautifulSoup(html, parser)|||        for tag in sp.find_all(\"a\"):|||            &lt;TO COMPLETE&gt;",
    "import base64||||||base64_img = 'iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAACXBIWXMAAAsTAAA' \|||             'LEwEAmpwYAAAB1klEQVQ4jY2TTUhUURTHf+fy/HrjhNEX2KRGiyIXg8xgSURuokX' \|||             'LxFW0qDTaSQupkHirthK0qF0WQQQR0UCbwCQyw8KCiDbShEYLJQdmpsk3895p4aS' \|||             'v92ass7pcfv/zP+fcc4U6kXKe2pTY3tjSUHjtnFgB0VqchC/SY8/293S23f+6VEj' \|||             '9KKwCoPDNIJdmr598GOZNJKNWTic7tqb27WwNuuwGvVWrAit84fsmMzE1P1+1TiK' \|||             'MVKvYUjdBvzPZXCwXzyhyWNBgVYkgrIow09VJMznpyebWE+Tdn9cEroBSc1JVPS+' \|||             '6moh5Xyjj65vEgBxafGzWetTh+rr1eE/c/TMYg8hlAOvI6JP4KmwLgJ4qD0TIbli' \|||             'TB+sunjkbeLekKsZ6Zc8V027aBRoBRHVoduDiSypmGFG7CrcBEyDHA0ZNfNphC0D' \|||             '6amYa6ANw3YbWD4Pn3oIc+EdL36V3od0A+MaMAXmA8x2Zyn+IQeQeBDfRcUw3B+2' \|||             'PxwZ/EdtTDpCPQLMh9TKx0k3pXipEVlknsf5KoNzGyOe1sz8nvYtTQT6yyvTjIax' \|||             'smHGB9pFx4n3jIEfDePQvCIrnn0J4B/gA5J4XcRfu4JZuRAw3C51OtOjM3l2bMb8' \|||             'Br5eXCsT/w/EAAAAASUVORK5CYII='|||||||||def image_decode(base64_img):|||    base64_img_bytes = base64_img.encode('utf-8')|||    with open('decoded_image.png', 'wb') as file_to_save:|||        &lt;TO COMPLETE&gt;",
    "import os|||import datetime|||||||||def has_date(text):|||    try:|||        datetime.datetime.strptime(text, '%d-%m-%Y')|||        return True|||    except ValueError:|||        return False|||||||||folder_path = \"/Users/User/Desktop\"|||||||||def print_names(folder_path):|||    # Recursively search in the directory|||    for root, d_names, f_names in os.walk(folder_path):|||        # We only focus on file names (no directories)|||        for name in f_names:|||            &lt;TO COMPLETE&gt;",
    "import speech_recognition as sr|||import moviepy.editor as mp|||from moviepy.video.io.ffmpeg_tools |||import ffmpeg_extract_subclip|||||||||def process_chunk(list_minutes, i):|||    ffmpeg_extract_subclip(\"my_video.mp4\", |||    list_minutes[i] - 2 * (list_minutes[i] != 0),|||    list_minutes[i + 1],targetname=|||    \"chunks/cut{}.mp4\".format(i + 1))|||    clip = mp.VideoFileClip(r\"chunks/cut{}.mp4\".format(i + 1))|||    clip.audio.write_audiofile(|||    r\"audios/converted{}.wav\".format(i + 1))|||    r = sr.Recognizer()|||    audio = sr.AudioFile(\"audios/converted{}.wav\"|||    .format(i + 1))|||    with audio as source:|||        r.adjust_for_ambient_noise(source)|||        audio_file = r.record(source)|||    result = r.recognize_google(audio_file)|||    return result|||||||||num_seconds_video = 52 * 60|||print(\"The video is {} seconds\".format(num_seconds_video))|||list_minutes = list(range(0, num_seconds_video + 1, 60))|||||||||def recognize_speech(list_minutes):|||    diz = {}|||    for i in range(len(list_minutes) - 1):|||        diz['chunk{}'.format(i + 1)] = |||        process_chunk(list_minutes, i)|||    l_chunks = [diz['chunk{}'.format(i + 1)] |||        for i in range(len(diz))]|||    text = '\n'.join(l_chunks)||||||    # you need to create a text document to store all the|||    # text that has been extracted from the video:|||    with open('recognized.txt', mode='w') as file:|||        &lt;TO COMPLETE&gt;"
];


table_data_validation=[
    "targetStream.download(filename=name)|||print(targetStream)|||targetStream.check_if_exists(video_url)",
    "airports_in_asia.append(line[1])|||airports_in_asia.add(line[0])|||airports_in_asia.append(line.substring(0,1))",
    "matrix [0, y] = y|||matrix [y, 0] = x|||matrix [y, 0] = x+y",
    "if distances[i] > maxDistance:|||if distances[i] > 0:|||if distances[i] > distances[i-1]:",
    "for i in range(len(words)):|||while to_return == \"\":|||for i in words:",
    "return num3|||return (num3 > num2)|||if num2 >= num&&& return num2",
    "mysock.close()|||mysock.disconnect((host, 80))|||return mysock",
    "cursor.execute(\"CREATE DATABASE experiment\")|||connector.run(\"CREATE DATABASE experiment\")|||cursor.create_table(\"experiment\")",
    "pdfkit.from_string(title, desktop_path + 'ICSE.pdf')|||html.create_pdf(title, 'desktop_path/ICSE.pdf')|||pdfkit.create(title, 'desktop_path/ICSE.pdf')",
    "ftp.quit()|||f.close()|||ftp.connect()",
    "split(' ')[1])|||split(',')[0])|||.join(','))",
    "str(i[0]).upper()|||str(i).upper()|||str(text[0]).upper()",
    "row+1):|||row):|||n_lines):",
    "in numbers:|||in range(1, len(numbers)):|||in numbers.split():",
    "csv_in_file:|||csv_in_file.split():|||range(1, len(csv_in_file)):",
    "data.decode('utf-8')|||data.send()|||resize()",
    "show()|||create_pdf(\"out.pdf\")|||write()",
    "ignores|||start_comment|||state",
    "readlines())|||readfirstline())|||flush())",
    "correct(), end=\" \", flush=True)|||TextBlob.correct(), end=\" \", flush=True)|||upper(), end=\" \", flush=True)",
    "tokens = str(image).split(\"/\")&&&only_name.append(tokens[len(tokens) - 1] + \".jpg\")|||tokens = str(image).split(\" \")&&&only_name.append(tokens[len(tokens)] + \".jpg\")|||image_name = str(image).split()[-1] + \".jpg\"&&&only_name.append(image_name)",
    "while len(self._observations) &gt; self._overlap:&&&    self._observations.pop(0)|||while len(self._observations) &gt; self._overlap:&&&    self._observations.pop(len(self._observations)-1)|||while len(self._observations) != self._overlap:&&&    self._observations.pop(0)",
    "sum += float(line[column_name])&&&count += 1|||count += float(line[column_name])&&&sum += count|||sum = line[column_name]&&&count = 1",
    "play_list.insert(pos, item)&&&pos += 1|||if item != play_list[pos]:&&&    continue&&&pos += 1|||play_list.insert(len(song_list) - pos, item)&&&pos += 1",
    "vid_src = article.find('iframe', class_='youtube-player')['src']&&&vid_id = vid_src.split('/')[4]&&&vid_id = vid_id.split('?')[0]&&&yt_link = f'https://youtube.com/watch?v={vid_id}'|||vid_src = article.find('iframe')['youtube-player']&&&vid_id = vid_id.split('?')[0]&&&yt_link = f'https://youtube.com/watch?v={vid_id}'|||vid_id = article.find('iframe', class_='youtube-player')['vid_id']&&&yt_link = f'https://youtube.com/watch?v={vid_id}'",
    "server.login(sender, password)&&&server.sendmail(sender, receive, message)|||server.sendmail(sender, receive, message)&&&smtplib.login(sender, password)|||smtplib.connect(sender, password, context, port, message)&&&smtplib.send()",
    "url = tag.get(\"href\")&&&if url is not None and \"articles\" in url:&&&    print(\"\n\" + url)|||url = tag.get(\"href\")&&&if url is not None and url == \"articles\":&&&    print(\"\n\" + url)|||href = tag.get(\"attribute\")&&&if href is not None and \"url\" in href:&&&    print(\"\n\" + href)",
    "decoded_image_data = base64.decodebytes(base64_img_bytes)&&&file_to_save.write(decoded_image_data)|||decoded_image_data = base64.decodebytes(base64_img_bytes)&&&file_to_save.load(decoded_image_data)|||decoded_image_data = base64_img_bytes.decodebytes(base64)&&&file_to_save.write(decoded_image_data)",
    "tokens = name.split(\".\")&&&name_no_extension = name.replace(\".\" + tokens[len(tokens)-1], \"\")&&&if has_date(name_no_extension):&&&    print(name_no_extension)|||name_no_extension = name.replace(\".\", \"\").substring(-3)&&&if has_date(name_no_extension):&&&    print(name_no_extension)|||tokens = name.split(\".\")&&&name_no_extension = name.replace(\"/\" + tokens[-1], \"\")&&&if has_date(name_no_extension):&&&    print(name_no_extension)",
    "file.write(\"Recognized Speech:\")&&&file.write(\"\n\")&&&file.write(text)&&&print(\"Finally ready!\")|||file.flush(\"Recognized Speech: % (text,))&&&print(\"Finally ready!\")|||file.read(\"Recognized Speech:\")&&&file.read(\"\n\")&&&file.read(text)&&&print(\"Finally ready!\")"



]

table_data_retrieval=[
    "def main_work_subdirs(gl):|||    for root, dirs, files in os.walk(gl['pwd']):|||        dirs.sort()|||        if root == gl['pwd']:|||            for d2i in dirs:|||                print(d2i)",
    "import  pandas as pd||||||df = pd.DataFrame([sub.split(\",\") for sub in l])|||print(df)||||||import  pandas as pd||||||df = pd.read_csv(\"in.csv\",skiprows=3,header=None)|||print(df)||||||df = pd.read_csv(\"in.csv\",header=None,comment=\"#\")  ||||||import pandas as pd|||from itertools import dropwhile|||import csv|||with open(\"in.csv\") as f:|||    f = dropwhile(lambda x: x.startswith(\"#!!\"), f)|||    r = csv.reader(f)|||    df = pd.DataFrame().from_records(r)",
    "import numpy as np|||import types|||from timeit import repeat||||||prom={np.dtype(np.int32): np.dtype(np.int64), np.dtype(float): np.dtype(float)}||||||def RI(k, N, dt, top=100):|||    return np.random.randint(0, top if top else N, (k, N)).astype(dt)||||||def RA(k, N, dt, top=None):|||    return np.add.outer(np.zeros((k,), int), np.arange(N)%(top if top else N)).astype(dt)||||||def RU(k, N, dt, top=100):|||    return (np.random.random((k, N))*(top if top else N)).astype(dt)||||||def data(k, N_b, N_e, dt_b, dt_e, b_fun=RI, e_fun=RA):|||    b = list(b_fun(k, N_b, dt_b))|||    e = list(e_fun(k, N_e, dt_e))|||    return b, e||||||def f_vander(b, e):|||    return np.vander(b, len(e), increasing=True)||||||def f_bc_b(b, e):|||    return b[:, None]**e||||||def f_bc_e(b, e):|||    return np.ascontiguousarray((b**e[:, None]).T)||||||def f_ma(b, e):|||    out = np.empty((len(b), len(e)), prom[b.dtype])|||    out[:, 0] = 1|||    np.multiply.accumulate(np.broadcast_to(b, (len(e)-1, len(b))), axis=0, out=out[:, 1:].T)|||    return out||||||def f_nnc(b, e):|||    out = np.empty((len(b), len(e)), prom[b.dtype])|||    out[:, 0] = 1|||    out[:, 1:] = b[:, None]|||    np.multiply.accumulate(out[:, 1:], out=out[:, 1:], axis=1)|||    return out||||||def f_out_e_1(b, e):|||    out = np.empty((len(b), len(e)), b.dtype)|||    out[:, 0] = 1|||    out[:, 1] = b|||    out[:, 2] = c = b*b|||    for i in range(3, len(e)):|||        c*=b|||        out[:, i] = c|||    return out||||||def f_out_e_2(b, e):|||    out = np.empty((len(b), len(e)), b.dtype)|||    out[:, 0] = 1|||    out[:, 1] = b|||    out[:, 2] = b*b|||    for i in range(3, len(e)):|||        out[:, i] = out[:, i-1] * b|||    return out||||||def f_safe_e_1(b, e):|||    out = np.empty((len(b), len(e)), prom[b.dtype])|||    out[:, 0] = 1|||    out[:, 1] = b|||    out[:, 2] = c = (b*b).astype(prom[b.dtype])|||    for i in range(3, len(e)):|||        c*=b|||        out[:, i] = c|||    return out||||||def f_safe_e_2(b, e):|||    out = np.empty((len(b), len(e)), prom[b.dtype])|||    out[:, 0] = 1|||    out[:, 1] = b|||    out[:, 2] = b*b|||    for i in range(3, len(e)):|||        out[:, i] = out[:, i-1] * b|||    return out||||||def f_bc_e_cheat(b, e):|||    return (b**e[:, None]).T||||||for params in [(100, 5000, 4, np.int32, np.int32),|||               (100, 5000, 8, np.int32, np.int32),|||               (100, 5000, 4, float, np.int32),|||               (100, 5000, 8, float, np.int32)]:|||    k = params[0]|||    dat = data(*params)|||    ref = f_vander(dat[0][0], dat[1][0])|||    print('rep={} n_b={} n_e={} b_tp={} e_tp={}'.format(*params))|||    for name, func in list(globals().items()):|||        if not name.startswith('f_') or not isinstance(func, types.FunctionType):|||            continue|||        try:|||            assert np.allclose(ref, func(dat[0][0], dat[1][0]))|||            if not func(dat[0][0], dat[1][0]).flags.c_contiguous:|||                print('cheat', end=' ')|||            print(\"{:16s}{:16.8f} ms\".format(name[2:], np.min(repeat(|||                'f(b.pop(), e.pop())', setup='b, e = data(*p)', globals={'f':func, 'data':data, 'p':params}, number=k)) * 1000 / k))|||        except:|||            print(\"{:16s} apparently failed\".format(name[2:]))",
    "import numpy as np|||import matplotlib.pyplot as plt|||import pandas as pd|||from skimage.measure import ransac, LineModelND, CircleModel|||import math  |||||||||df = pd.read_csv('scanData.txt',delimiter=',')|||angle = df.values[:,0]|||distance = df.values[:,1]||||||x= angle|||y= distance||||||cartesian = [(r*math.cos(phi*math.pi/180), r*math.sin(phi*math.pi/180)) for r, |||             phi in zip(distance, angle)]||||||x, y = map(list, zip(*cartesian))||||||# coverting this into 2d array|||x=  np.array(x)|||y=  np.array(y)||||||x=x.reshape(-1, 1)|||y=y.reshape(-1, 1)||||||data = np.column_stack([x, y])||||||model = LineModelND()|||model.estimate(data)|||# robustly fit line only using inlier data with RANSAC algorithm|||model_robust, inliers = ransac(data, LineModelND, min_samples=2,|||                               residual_threshold=10, max_trials=1000)|||outliers = inliers == False||||||# generate coordinates of estimated models|||line_x = np.arange(x.min(),x.max())  #[:, np.newaxis]|||line_y = model.predict_y(line_x)|||line_y_robust = model_robust.predict_y(line_x)||||||fig, ax = plt.subplots()|||ax.plot(data[outliers, 0], data[outliers, 1], '.r', alpha=0.6,|||        label='Outlier data')|||ax.plot(data[inliers, 0], data[inliers, 1], '.b', alpha=0.6,|||        label='Inlier data')|||print(\"data: \", data)|||print(data[inliers, 0], data[inliers, 1])|||#ax.plot(line_x, line_y, '-k', label='Line model from all data')|||#ax.plot(line_x, line_y_robust, '-b', label='Robust line model')|||#ax.legend(loc='lower left')|||plt.show()",
    "'.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(N))||||||''.join(random.choices(string.ascii_uppercase + string.digits, k=N))||||||''.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(N))||||||>>> import string|||>>> import random|||>>> def id_generator(size=6, chars=string.ascii_uppercase + string.digits):|||...    return ''.join(random.choice(chars) for _ in range(size))|||...|||>>> id_generator()|||'G5G74W'|||>>> id_generator(3, \"6793YUIO\")|||'Y3U'||||||>>> string.ascii_uppercase|||'ABCDEFGHIJKLMNOPQRSTUVWXYZ'|||>>> string.digits|||'0123456789'|||>>> string.ascii_uppercase + string.digits|||'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'||||||>>> range(4) # range create a list of 'n' numbers|||[0, 1, 2, 3]|||>>> ['elem' for _ in range(4)] # we use range to create 4 times 'elem'|||['elem', 'elem', 'elem', 'elem']||||||>>> random.choice(\"abcde\")|||'a'|||>>> random.choice(\"abcde\")|||'d'|||>>> random.choice(\"abcde\")|||'b'||||||>>> [random.choice('abcde') for _ in range(3)]|||['a', 'b', 'b']|||>>> [random.choice('abcde') for _ in range(3)]|||['e', 'b', 'e']|||>>> [random.choice('abcde') for _ in range(3)]|||['d', 'a', 'c']||||||>>> ''.join(['a', 'b', 'b'])|||'abb'|||>>> [random.choice('abcde') for _ in range(3)]|||['d', 'c', 'b']|||>>> ''.join(random.choice('abcde') for _ in range(3))|||'dac'",
    "for idx, val in enumerate(ints):|||    print(idx, val)",
    "import supervisor.xmlrpc|||import xmlrpclib||||||proxy = xmlrpclib.ServerProxy('http://127.0.0.1',|||                               transport=supervisor.xmlrpc.SupervisorTransport(|||                                    None, None, serverurl='unix://'+socketpath))||||||proxy.supervisor.getState()||||||class UnixStreamHTTPConnection(httplib.HTTPConnection, object):|||    def __init__(self, *args, **kwargs):|||        self.socketpath = kwargs.pop('socketpath')|||        super(UnixStreamHTTPConnection, self).__init__(*args, **kwargs)||||||    def connect(self):|||        self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)|||        self.sock.connect_ex(self.socketpath)||||||class UnixStreamTransport(xmlrpclib.Transport, object):|||    def __init__(self, *args, **kwargs):|||        self.socketpath = kwargs.pop('socketpath')|||        super(UnixStreamTransport, self).__init__(*args, **kwargs)",
    "python setup.py build|||python setup.py install||||||import pymysql|||import pymysql.cursors|||conn= pymysql.connect(host='localhost',user='user',password='user',db='testdb',charset='utf8mb4',cursorclass=pymysql.cursors.DictCursor)|||a=conn.cursor()|||sql='CREATE TABLE `users` (`id` int(11) NOT NULL AUTO_INCREMENT,`email` varchar(255) NOT NULL,`password` varchar(255) NOT NULL,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 ;'|||a.execute(sql)",
    "from unidecode import unidecode|||print unidecode(u\"\u5317\u4EB0\")",
    "f = open (locfile, \"rb\")",
    "# with is like your try .. finally block in this case|||with open('stats.txt', 'r') as file:|||    # read a list of lines into data|||    data = file.readlines()||||||print data|||print \"Your name: \" + data[0]||||||# now change the 2nd line, note that you have to add a newline|||data[1] = 'Mage\n'||||||# and write everything back|||with open('stats.txt', 'w') as file:|||    file.writelines( data )",
    "fileList = ['FileName1.log','FileName2.log']||||||fileToRead=fileList[userInput]||||||f=open(fileToRead, 'r')",
    "&lt;factorial&gt; = lambda a, b: b*a(a, b-1) if b &gt; 0 else 1||||||&lt;factorial-application&gt; = (lambda a, b: a(a, b))(&lt;factorial&gt;, b)||||||recursive_lambda = (lambda func: lambda *args: func(func, *args))|||print(recursive_lambda(lambda self, x: x * self(self, x - 1) if x &gt; 0 else 1)(6))|||# Or, using the function verbatim:|||print(recursive_lambda(lambda a, b: b*a(a, b-1) if b &gt; 0 else 1)(6))||||||(lambda b: &lt;factorial-application&gt;)(num)||||||fact = lambda x: 1 if x == 0 else x * fact(x-1)||||||def recursive_lambda(func):|||    def ret(*args):|||        return func(ret, *args)|||    return ret||||||print(recursive_lambda(lambda factorial, x: x * factorial(x - 1) if x &gt; 1 else 1)(6))  # 720",
    "lst = [1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]||||||index = 0|||leftCount, rightCount = 0, lst.count(0)|||while leftCount &lt; rightCount:|||    if lst[index] == 1:|||        leftCount += 1|||    else:|||        rightCount -= 1|||    index += 1||||||print(lst[:index], index, lst[index + 1:])",
    "import csv|||from itertools import count||||||with open('MySpreadsheet.csv', 'rU') as f:|||    reader = csv.DictReader(f, dialect=csv.excel)|||    row_count = count(1)||||||    for row in reader:|||        next(count)|||        print(row)||||||for i in range(row_count):|||    print('Stack Overflow')||||||with open('MySpreadsheet.csv', 'rU') as f:|||    reader = csv.DictReader(f, dialect=csv.excel)||||||    for row in reader:|||        print(row)||||||with open('MySpreadsheet.csv', 'rU') as f:|||    reader = csv.DictReader(f, dialect=csv.excel)||||||    for row in reader:|||        print('Stack Overflow')||||||with open('MySpreadsheet.csv', 'rU') as f:|||    reader = csv.DictReader(f, dialect=csv.excel)||||||    for row in reader:|||        print(row)||||||    f.seek(0)|||    next(reader)||||||    for row in reader:|||        print('Stack Overflow')",
    "import logging|||import logging.handlers|||import os|||import time|||import sys||||||import cv2|||import numpy as np||||||from vehicle_counter import VehicleCounter||||||# ============================================================================||||||IMAGE_DIR = \"images\"|||IMAGE_FILENAME_FORMAT = IMAGE_DIR + \"/frame_%04d.png\"||||||# Support either video file or individual frames|||CAPTURE_FROM_VIDEO = False|||if CAPTURE_FROM_VIDEO:|||    IMAGE_SOURCE = \"traffic.avi\" # Video file|||else:|||    IMAGE_SOURCE = IMAGE_FILENAME_FORMAT # Image sequence||||||# Time to wait between frames, 0=forever|||WAIT_TIME = 1 # 250 # ms||||||LOG_TO_FILE = True||||||# Colours for drawing on processed frames    |||DIVIDER_COLOUR = (255, 255, 0)|||BOUNDING_BOX_COLOUR = (255, 0, 0)|||CENTROID_COLOUR = (0, 0, 255)||||||# ============================================================================||||||def init_logging():|||    main_logger = logging.getLogger()||||||    formatter = logging.Formatter(|||        fmt='%(asctime)s.%(msecs)03d %(levelname)-8s [%(name)s] %(message)s'|||        , datefmt='%Y-%m-%d %H:%M:%S')||||||    handler_stream = logging.StreamHandler(sys.stdout)|||    handler_stream.setFormatter(formatter)|||    main_logger.addHandler(handler_stream)||||||    if LOG_TO_FILE:|||        handler_file = logging.handlers.RotatingFileHandler(\"debug.log\"|||            , maxBytes = 2**24|||            , backupCount = 10)|||        handler_file.setFormatter(formatter)|||        main_logger.addHandler(handler_file)||||||    main_logger.setLevel(logging.DEBUG)||||||    return main_logger||||||# ============================================================================||||||def save_frame(file_name_format, frame_number, frame, label_format):|||    file_name = file_name_format % frame_number|||    label = label_format % frame_number||||||    log.debug(\"Saving %s as '%s'\", label, file_name)|||    cv2.imwrite(file_name, frame)||||||# ============================================================================||||||def get_centroid(x, y, w, h):|||    x1 = int(w / 2)|||    y1 = int(h / 2)||||||    cx = x + x1|||    cy = y + y1||||||    return (cx, cy)||||||# ============================================================================||||||def detect_vehicles(fg_mask):|||    log = logging.getLogger(\"detect_vehicles\")||||||    MIN_CONTOUR_WIDTH = 21|||    MIN_CONTOUR_HEIGHT = 21||||||    # Find the contours of any vehicles in the image|||    contours, hierarchy = cv2.findContours(fg_mask|||        , cv2.RETR_EXTERNAL|||        , cv2.CHAIN_APPROX_SIMPLE)||||||    log.debug(\"Found %d vehicle contours.\", len(contours))||||||    matches = []|||    for (i, contour) in enumerate(contours):|||        (x, y, w, h) = cv2.boundingRect(contour)|||        contour_valid = (w &gt;= MIN_CONTOUR_WIDTH) and (h &gt;= MIN_CONTOUR_HEIGHT)||||||        log.debug(\"Contour #%d: pos=(x=%d, y=%d) size=(w=%d, h=%d) valid=%s\"|||            , i, x, y, w, h, contour_valid)||||||        if not contour_valid:|||            continue||||||        centroid = get_centroid(x, y, w, h)||||||        matches.append(((x, y, w, h), centroid))||||||    return matches||||||# ============================================================================||||||def filter_mask(fg_mask):|||    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))||||||    # Fill any small holes|||    closing = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)|||    # Remove noise|||    opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)||||||    # Dilate to merge adjacent blobs|||    dilation = cv2.dilate(opening, kernel, iterations = 2)||||||    return dilation||||||# ============================================================================||||||def process_frame(frame_number, frame, bg_subtractor, car_counter):|||    log = logging.getLogger(\"process_frame\")||||||    # Create a copy of source frame to draw into|||    processed = frame.copy()||||||    # Draw dividing line -- we count cars as they cross this line.|||    cv2.line(processed, (0, car_counter.divider), (frame.shape[1], car_counter.divider), DIVIDER_COLOUR, 1)||||||    # Remove the background|||    fg_mask = bg_subtractor.apply(frame, None, 0.01)|||    fg_mask = filter_mask(fg_mask)||||||    save_frame(IMAGE_DIR + \"/mask_%04d.png\"|||        , frame_number, fg_mask, \"foreground mask for frame #%d\")||||||    matches = detect_vehicles(fg_mask)||||||    log.debug(\"Found %d valid vehicle contours.\", len(matches))|||    for (i, match) in enumerate(matches):|||        contour, centroid = match||||||        log.debug(\"Valid vehicle contour #%d: centroid=%s, bounding_box=%s\", i, centroid, contour)||||||        x, y, w, h = contour||||||        # Mark the bounding box and the centroid on the processed frame|||        # NB: Fixed the off-by one in the bottom right corner|||        cv2.rectangle(processed, (x, y), (x + w - 1, y + h - 1), BOUNDING_BOX_COLOUR, 1)|||        cv2.circle(processed, centroid, 2, CENTROID_COLOUR, -1)||||||    log.debug(\"Updating vehicle count...\")|||    car_counter.update_count(matches, processed)||||||    return processed||||||# ============================================================================||||||def main():|||    log = logging.getLogger(\"main\")||||||    log.debug(\"Creating background subtractor...\")|||    bg_subtractor = cv2.BackgroundSubtractorMOG()||||||    log.debug(\"Pre-training the background subtractor...\")|||    default_bg = cv2.imread(IMAGE_FILENAME_FORMAT % 119)|||    bg_subtractor.apply(default_bg, None, 1.0)||||||    car_counter = None # Will be created after first frame is captured||||||    # Set up image source|||    log.debug(\"Initializing video capture device #%s...\", IMAGE_SOURCE)|||    cap = cv2.VideoCapture(IMAGE_SOURCE)||||||    frame_width = cap.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH)|||    frame_height = cap.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT)|||    log.debug(\"Video capture frame size=(w=%d, h=%d)\", frame_width, frame_height)||||||    log.debug(\"Starting capture loop...\")|||    frame_number = -1|||    while True:|||        frame_number += 1|||        log.debug(\"Capturing frame #%d...\", frame_number)|||        ret, frame = cap.read()|||        if not ret:|||            log.error(\"Frame capture failed, stopping...\")|||            break||||||        log.debug(\"Got frame #%d: shape=%s\", frame_number, frame.shape)||||||        if car_counter is None:|||            # We do this here, so that we can initialize with actual frame size|||            log.debug(\"Creating vehicle counter...\")|||            car_counter = VehicleCounter(frame.shape[:2], frame.shape[0] / 2)||||||        # Archive raw frames from video to disk for later inspection/testing|||        if CAPTURE_FROM_VIDEO:|||            save_frame(IMAGE_FILENAME_FORMAT|||                , frame_number, frame, \"source frame #%d\")||||||        log.debug(\"Processing frame #%d...\", frame_number)|||        processed = process_frame(frame_number, frame, bg_subtractor, car_counter)||||||        save_frame(IMAGE_DIR + \"/processed_%04d.png\"|||            , frame_number, processed, \"processed frame #%d\")||||||        cv2.imshow('Source Image', frame)|||        cv2.imshow('Processed Image', processed)||||||        log.debug(\"Frame #%d processed.\", frame_number)||||||        c = cv2.waitKey(WAIT_TIME)|||        if c == 27:|||            log.debug(\"ESC detected, stopping...\")|||            break||||||    log.debug(\"Closing video capture device...\")|||    cap.release()|||    cv2.destroyAllWindows()|||    log.debug(\"Done.\")||||||# ============================================================================||||||if __name__ == \"__main__\":|||    log = init_logging()||||||    if not os.path.exists(IMAGE_DIR):|||        log.debug(\"Creating image directory `%s`...\", IMAGE_DIR)|||        os.makedirs(IMAGE_DIR)||||||    main()||||||import logging||||||# ============================================================================||||||class VehicleCounter(object):|||    def __init__(self, shape, divider):|||        self.log = logging.getLogger(\"vehicle_counter\")||||||        self.height, self.width = shape|||        self.divider = divider||||||        self.vehicle_count = 0|||||||||    def update_count(self, matches, output_image = None):|||        self.log.debug(\"Updating count using %d matches...\", len(matches))||||||# ============================================================================||||||INPUT_WIDTH = 160|||INPUT_HEIGHT = 120||||||OUTPUT_TILE_WIDTH = 10|||OUTPUT_TILE_HEIGHT = 12||||||TILE_COUNT = OUTPUT_TILE_WIDTH * OUTPUT_TILE_HEIGHT||||||# ============================================================================||||||def stitch_images(input_format, output_filename):|||    output_shape = (INPUT_HEIGHT * OUTPUT_TILE_HEIGHT|||        , INPUT_WIDTH * OUTPUT_TILE_WIDTH|||        , 3)|||    output = np.zeros(output_shape, np.uint8)||||||    for i in range(TILE_COUNT):|||        img = cv2.imread(input_format % i)|||        cv2.rectangle(img, (0, 0), (INPUT_WIDTH - 1, INPUT_HEIGHT - 1), (0, 0, 255), 1)|||        # Draw the frame number|||        cv2.putText(img, str(i), (2, 10)|||            , cv2.FONT_HERSHEY_PLAIN, 0.7, (255, 255, 255), 1)|||        x = i % OUTPUT_TILE_WIDTH * INPUT_WIDTH|||        y = i / OUTPUT_TILE_WIDTH * INPUT_HEIGHT|||        output[y:y+INPUT_HEIGHT, x:x+INPUT_WIDTH,:] = img||||||    cv2.imwrite(output_filename, output)||||||# ============================================================================||||||stitch_images(\"images/frame_%04d.png\", \"stitched_frames.png\")|||stitch_images(\"images/mask_%04d.png\", \"stitched_masks.png\")|||stitch_images(\"images/processed_%04d.png\", \"stitched_processed.png\")|||||||||bg_subtractor = cv2.BackgroundSubtractorMOG()|||default_bg = cv2.imread(IMAGE_FILENAME_FORMAT % 119)|||bg_subtractor.apply(default_bg, None, 1.0)||||||def filter_mask(fg_mask):|||    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))||||||    # Fill any small holes|||    closing = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)|||    # Remove noise|||    opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)||||||    # Dilate to merge adjacent blobs|||    dilation = cv2.dilate(opening, kernel, iterations = 2)||||||    return dilation||||||traces = {|||    'A': [(112, 36), (112, 45), (112, 52), (112, 54), (112, 63), (111, 73), (111, 86), (111, 91), (111, 97), (110, 105)]|||    , 'B': [(119, 37), (120, 42), (121, 54), (121, 55), (123, 64), (124, 74), (125, 87), (127, 94), (125, 100), (126, 108)]|||    , 'C': [(93, 23), (91, 27), (89, 31), (87, 36), (85, 42), (82, 49), (79, 59), (74, 71), (70, 82), (62, 86), (61, 92), (55, 101)]|||    , 'D': [(118, 30), (124, 83), (125, 90), (116, 101), (122, 100)]|||    , 'E': [(77, 27), (75, 30), (73, 33), (70, 37), (67, 42), (63, 47), (59, 53), (55, 59), (49, 67), (43, 75), (36, 85), (27, 92), (24, 97), (20, 102)]|||    , 'F': [(119, 30), (120, 34), (120, 39), (122, 59), (123, 60), (124, 70), (125, 82), (127, 91), (126, 97), (128, 104)]|||    , 'G': [(88, 37), (87, 41), (85, 48), (82, 55), (79, 63), (76, 74), (72, 87), (67, 92), (65, 98), (60, 106)]|||    , 'H': [(124, 35), (123, 40), (125, 45), (127, 59), (126, 59), (128, 67), (130, 78), (132, 88), (134, 93), (135, 99), (135, 107)]|||    , 'I': [(98, 26), (97, 30), (96, 34), (94, 40), (92, 47), (90, 55), (87, 64), (84, 77), (79, 87), (74, 93), (73, 102)]|||    , 'J': [(123, 60), (125, 63), (125, 81), (127, 93), (126, 98), (125, 100)]|||}||||||def get_vector(a, b):|||    \"\"\"Calculate vector (distance, angle in degrees) from point a to point b.||||||    Angle ranges from -180 to 180 degrees.|||    Vector with angle 0 points straight down on the image.|||    Values increase in clockwise direction.|||    \"\"\"|||    dx = float(b[0] - a[0])|||    dy = float(b[1] - a[1])||||||    distance = math.sqrt(dx**2 + dy**2)||||||    if dy &gt; 0:|||        angle = math.degrees(math.atan(-dx/dy))|||    elif dy == 0:|||        if dx &lt; 0:|||            angle = 90.0|||        elif dx &gt; 0:|||            angle = -90.0|||        else:|||            angle = 0.0|||    else:|||        if dx &lt; 0:|||            angle = 180 - math.degrees(math.atan(dx/dy))|||        elif dx &gt; 0:|||            angle = -180 - math.degrees(math.atan(dx/dy))|||        else:|||            angle = 180.0        ||||||    return distance, angle||||||def is_valid_vector(a):|||    distance, angle = a|||    threshold_distance = max(10.0, -0.008 * angle**2 + 0.4 * angle + 25.0)|||    return (distance &lt;= threshold_distance)||||||import logging|||import math||||||import cv2|||import numpy as np||||||# ============================================================================||||||CAR_COLOURS = [ (0,0,255), (0,106,255), (0,216,255), (0,255,182), (0,255,76)|||    , (144,255,0), (255,255,0), (255,148,0), (255,0,178), (220,0,255) ]||||||# ============================================================================||||||class Vehicle(object):|||    def __init__(self, id, position):|||        self.id = id|||        self.positions = [position]|||        self.frames_since_seen = 0|||        self.counted = False||||||    @property|||    def last_position(self):|||        return self.positions[-1]||||||    def add_position(self, new_position):|||        self.positions.append(new_position)|||        self.frames_since_seen = 0||||||    def draw(self, output_image):|||        car_colour = CAR_COLOURS[self.id % len(CAR_COLOURS)]|||        for point in self.positions:|||            cv2.circle(output_image, point, 2, car_colour, -1)|||            cv2.polylines(output_image, [np.int32(self.positions)]|||                , False, car_colour, 1)|||||||||# ============================================================================||||||class VehicleCounter(object):|||    def __init__(self, shape, divider):|||        self.log = logging.getLogger(\"vehicle_counter\")||||||        self.height, self.width = shape|||        self.divider = divider||||||        self.vehicles = []|||        self.next_vehicle_id = 0|||        self.vehicle_count = 0|||        self.max_unseen_frames = 7|||||||||    @staticmethod|||    def get_vector(a, b):|||        \"\"\"Calculate vector (distance, angle in degrees) from point a to point b.||||||        Angle ranges from -180 to 180 degrees.|||        Vector with angle 0 points straight down on the image.|||        Values increase in clockwise direction.|||        \"\"\"|||        dx = float(b[0] - a[0])|||        dy = float(b[1] - a[1])||||||        distance = math.sqrt(dx**2 + dy**2)||||||        if dy &gt; 0:|||            angle = math.degrees(math.atan(-dx/dy))|||        elif dy == 0:|||            if dx &lt; 0:|||                angle = 90.0|||            elif dx &gt; 0:|||                angle = -90.0|||            else:|||                angle = 0.0|||        else:|||            if dx &lt; 0:|||                angle = 180 - math.degrees(math.atan(dx/dy))|||            elif dx &gt; 0:|||                angle = -180 - math.degrees(math.atan(dx/dy))|||            else:|||                angle = 180.0        ||||||        return distance, angle |||||||||    @staticmethod|||    def is_valid_vector(a):|||        distance, angle = a|||        threshold_distance = max(10.0, -0.008 * angle**2 + 0.4 * angle + 25.0)|||        return (distance &lt;= threshold_distance)|||||||||    def update_vehicle(self, vehicle, matches):|||        # Find if any of the matches fits this vehicle|||        for i, match in enumerate(matches):|||            contour, centroid = match||||||            vector = self.get_vector(vehicle.last_position, centroid)|||            if self.is_valid_vector(vector):|||                vehicle.add_position(centroid)|||                self.log.debug(\"Added match (%d, %d) to vehicle #%d. vector=(%0.2f,%0.2f)\"|||                    , centroid[0], centroid[1], vehicle.id, vector[0], vector[1])|||                return i||||||        # No matches fit...        |||        vehicle.frames_since_seen += 1|||        self.log.debug(\"No match for vehicle #%d. frames_since_seen=%d\"|||            , vehicle.id, vehicle.frames_since_seen)||||||        return None|||||||||    def update_count(self, matches, output_image = None):|||        self.log.debug(\"Updating count using %d matches...\", len(matches))||||||        # First update all the existing vehicles|||        for vehicle in self.vehicles:|||            i = self.update_vehicle(vehicle, matches)|||            if i is not None:|||                del matches[i]||||||        # Add new vehicles based on the remaining matches|||        for match in matches:|||            contour, centroid = match|||            new_vehicle = Vehicle(self.next_vehicle_id, centroid)|||            self.next_vehicle_id += 1|||            self.vehicles.append(new_vehicle)|||            self.log.debug(\"Created new vehicle #%d from match (%d, %d).\"|||                , new_vehicle.id, centroid[0], centroid[1])||||||        # Count any uncounted vehicles that are past the divider|||        for vehicle in self.vehicles:|||            if not vehicle.counted and (vehicle.last_position[1] &gt; self.divider):|||                self.vehicle_count += 1|||                vehicle.counted = True|||                self.log.debug(\"Counted vehicle #%d (total count=%d).\"|||                    , vehicle.id, self.vehicle_count)||||||        # Optionally draw the vehicles on an image|||        if output_image is not None:|||            for vehicle in self.vehicles:|||                vehicle.draw(output_image)||||||            cv2.putText(output_image, (\"%02d\" % self.vehicle_count), (142, 10)|||                , cv2.FONT_HERSHEY_PLAIN, 0.7, (127, 255, 255), 1)||||||        # Remove vehicles that have not been seen long enough|||        removed = [ v.id for v in self.vehicles|||            if v.frames_since_seen &gt;= self.max_unseen_frames ]|||        self.vehicles[:] = [ v for v in self.vehicles|||            if not v.frames_since_seen &gt;= self.max_unseen_frames ]|||        for id in removed:|||            self.log.debug(\"Removed vehicle #%d.\", id)||||||        self.log.debug(\"Count updated, tracking %d vehicles.\", len(self.vehicles))||||||# ============================================================================",
    "plt.show()||||||plt.imshow(img.reshape((28, 28)))|||plt.show()",
    "def stateful_cut(arr, batch_size, T_after_cut):|||    if len(arr.shape) != 3:|||        # N: Independent sample size,|||        # T: Time length,|||        # m: Dimension|||        print(\"ERROR: please format arr as a (N, T, m) array.\")||||||    N = arr.shape[0]|||    T = arr.shape[1]||||||    # We need T_after_cut * nb_cuts = T|||    nb_cuts = int(T / T_after_cut)|||    if nb_cuts * T_after_cut != T:|||        print(\"ERROR: T_after_cut must divide T\")||||||    # We need batch_size * nb_reset = N|||    # If nb_reset = 1, we only reset after the whole epoch, so no need to reset|||    nb_reset = int(N / batch_size)|||    if nb_reset * batch_size != N:|||        print(\"ERROR: batch_size must divide N\")||||||    # Cutting (technical)|||    cut1 = np.split(arr, nb_reset, axis=0)|||    cut2 = [np.split(x, nb_cuts, axis=1) for x in cut1]|||    cut3 = [np.concatenate(x) for x in cut2]|||    cut4 = np.concatenate(cut3)|||    return(cut4)||||||import numpy as np|||from keras.models import Sequential|||from keras.layers import Dense, LSTM, TimeDistributed|||import matplotlib.pyplot as plt|||import matplotlib.patches as mpatches||||||##|||# Data|||##|||N = X_train.shape[0] # size of samples|||T = X_train.shape[1] # length of each time series|||batch_size = N # number of time series considered together: batch_size | N|||T_after_cut = 100 # length of each cut part of the time series: T_after_cut | T|||dim_in = X_train.shape[2] # dimension of input time series|||dim_out = y_train.shape[2] # dimension of output time series||||||inputs, outputs, inputs_test, outputs_test = \|||  [stateful_cut(arr, batch_size, T_after_cut) for arr in \|||  [X_train, y_train, X_test, y_test]]||||||##|||# Model|||##|||nb_units = 10||||||model = Sequential()|||model.add(LSTM(batch_input_shape=(batch_size, None, dim_in),|||               return_sequences=True, units=nb_units, stateful=True))|||model.add(TimeDistributed(Dense(activation='linear', units=dim_out)))|||model.compile(loss = 'mse', optimizer = 'rmsprop')|||We train the model without resetting states. We can do it only because we have selected batch_size = N.||||||##|||# Training|||##|||epochs = 100||||||nb_reset = int(N / batch_size)|||if nb_reset &gt; 1:|||    print(\"ERROR: We need to reset states when batch_size &lt; N\")||||||# When nb_reset = 1, we do not need to reinitialize states|||history = model.fit(inputs, outputs, epochs = epochs, |||                    batch_size = batch_size, shuffle=False,|||                    validation_data=(inputs_test, outputs_test))||||||## Mime model which is stateless but containing stateful weights|||model_stateless = Sequential()|||model_stateless.add(LSTM(input_shape=(None, dim_in),|||               return_sequences=True, units=nb_units))|||model_stateless.add(TimeDistributed(Dense(activation='linear', units=dim_out)))|||model_stateless.compile(loss = 'mse', optimizer = 'rmsprop')|||model_stateless.set_weights(model.get_weights())||||||################|||# Code from OP #|||################|||import numpy as np|||def random_sample(len_timeseries=3000):|||    Nchoice = 600|||    x1 = np.cos(np.arange(0,len_timeseries)/float(1.0 + np.random.choice(Nchoice)))|||    x2 = np.cos(np.arange(0,len_timeseries)/float(1.0 + np.random.choice(Nchoice)))|||    x3 = np.sin(np.arange(0,len_timeseries)/float(1.0 + np.random.choice(Nchoice)))|||    x4 = np.sin(np.arange(0,len_timeseries)/float(1.0 + np.random.choice(Nchoice)))|||    y1 = np.random.random(len_timeseries)|||    y2 = np.random.random(len_timeseries)|||    y3 = np.random.random(len_timeseries)|||    for t in range(3,len_timeseries):|||        ## the output time series depend on input as follows: |||        y1[t] = x1[t-2] |||        y2[t] = x2[t-1]*x3[t-2]|||        y3[t] = x4[t-3]|||    y = np.array([y1,y2,y3]).T|||    X = np.array([x1,x2,x3,x4]).T|||    return y, X|||def generate_data(Nsequence = 1000):|||    X_train = []|||    y_train = []|||    for isequence in range(Nsequence):|||        y, X = random_sample()|||        X_train.append(X)|||        y_train.append(y)|||    return np.array(X_train),np.array(y_train)||||||Nsequence = 100|||prop = 0.5|||Ntrain = int(Nsequence*prop)|||X, y = generate_data(Nsequence)|||X_train = X[:Ntrain,:,:]|||X_test  = X[Ntrain:,:,:]|||y_train = y[:Ntrain,:,:]|||y_test  = y[Ntrain:,:,:] ||||||#X.shape = (N sequence, length of time series, N input features)|||#y.shape = (N sequence, length of time series, N targets)|||print(X.shape, y.shape)|||# (100, 3000, 4) (100, 3000, 3)||||||####################|||# Cutting function #|||####################|||def stateful_cut(arr, batch_size, T_after_cut):|||    if len(arr.shape) != 3:|||        # N: Independent sample size,|||        # T: Time length,|||        # m: Dimension|||        print(\"ERROR: please format arr as a (N, T, m) array.\")||||||    N = arr.shape[0]|||    T = arr.shape[1]||||||    # We need T_after_cut * nb_cuts = T|||    nb_cuts = int(T / T_after_cut)|||    if nb_cuts * T_after_cut != T:|||        print(\"ERROR: T_after_cut must divide T\")||||||    # We need batch_size * nb_reset = N|||    # If nb_reset = 1, we only reset after the whole epoch, so no need to reset|||    nb_reset = int(N / batch_size)|||    if nb_reset * batch_size != N:|||        print(\"ERROR: batch_size must divide N\")||||||    # Cutting (technical)|||    cut1 = np.split(arr, nb_reset, axis=0)|||    cut2 = [np.split(x, nb_cuts, axis=1) for x in cut1]|||    cut3 = [np.concatenate(x) for x in cut2]|||    cut4 = np.concatenate(cut3)|||    return(cut4)||||||#############|||# Main code #|||#############|||from keras.models import Sequential|||from keras.layers import Dense, LSTM, TimeDistributed|||import matplotlib.pyplot as plt|||import matplotlib.patches as mpatches||||||##|||# Data|||##|||N = X_train.shape[0] # size of samples|||T = X_train.shape[1] # length of each time series|||batch_size = N # number of time series considered together: batch_size | N|||T_after_cut = 100 # length of each cut part of the time series: T_after_cut | T|||dim_in = X_train.shape[2] # dimension of input time series|||dim_out = y_train.shape[2] # dimension of output time series||||||inputs, outputs, inputs_test, outputs_test = \|||  [stateful_cut(arr, batch_size, T_after_cut) for arr in \|||  [X_train, y_train, X_test, y_test]]||||||##|||# Model|||##|||nb_units = 10||||||model = Sequential()|||model.add(LSTM(batch_input_shape=(batch_size, None, dim_in),|||               return_sequences=True, units=nb_units, stateful=True))|||model.add(TimeDistributed(Dense(activation='linear', units=dim_out)))|||model.compile(loss = 'mse', optimizer = 'rmsprop')||||||##|||# Training|||##|||epochs = 100||||||nb_reset = int(N / batch_size)|||if nb_reset &gt; 1:|||    print(\"ERROR: We need to reset states when batch_size &lt; N\")||||||# When nb_reset = 1, we do not need to reinitialize states|||history = model.fit(inputs, outputs, epochs = epochs, |||                    batch_size = batch_size, shuffle=False,|||                    validation_data=(inputs_test, outputs_test))||||||def plotting(history):|||    plt.plot(history.history['loss'], color = \"red\")|||    plt.plot(history.history['val_loss'], color = \"blue\")|||    red_patch = mpatches.Patch(color='red', label='Training')|||    blue_patch = mpatches.Patch(color='blue', label='Test')|||    plt.legend(handles=[red_patch, blue_patch])|||    plt.xlabel('Epochs')|||    plt.ylabel('MSE loss')|||    plt.show()||||||plt.figure(figsize=(10,8))|||plotting(history) # Evolution of training/test loss||||||##|||# Visual checking for a time series|||##|||## Mime model which is stateless but containing stateful weights|||model_stateless = Sequential()|||model_stateless.add(LSTM(input_shape=(None, dim_in),|||               return_sequences=True, units=nb_units))|||model_stateless.add(TimeDistributed(Dense(activation='linear', units=dim_out)))|||model_stateless.compile(loss = 'mse', optimizer = 'rmsprop')|||model_stateless.set_weights(model.get_weights())||||||## Prediction of a new set|||i = 0 # time series selected (between 0 and N-1)|||x = X_train[i]|||y = y_train[i]|||y_hat = model_stateless.predict(np.array([x]))[0]||||||for dim in range(3): # dim = 0 for y1 ; dim = 1 for y2 ; dim = 2 for y3.|||    plt.figure(figsize=(10,8))|||    plt.plot(range(T), y[:,dim])|||    plt.plot(range(T), y_hat[:,dim])|||    plt.show()||||||## Conclusion: works almost perfectly.",
    "def main_work_subdirs(gl):|||    for root, dirs, files in os.walk(gl['pwd']):|||        dirs.sort()|||        if root == gl['pwd']:|||            for d2i in dirs:|||                print(d2i)",
    "### If you just want a list of words|||def find_keyword_matches(sentence, keyword_list):|||    s1 = sentence.split(' ')|||    return [i for i in  s1 if i in keyword_list]||||||find_keyword_matches(sentence2, comorbidity_keywords)||||||['segmentectomy']||||||def find_keyword_matches(sentence, keyword_list):|||    s1 = sentence.split(' ')|||    return {xyz.index(i):i for i in xyz if i in comorbidity_keywords}||||||{17: 'segmentectomy'}||||||def word_range(sentence, keyword):|||    try:|||        idx_start = sentence.index(keyword)|||        idx_end = idx_start + len(keyword)|||        print(f'Word \'{keyword}\' found within index range {idx_start} to {idx_end}')|||        if idx_start &gt; 0:|||            return keyword|||    except ValueError:|||        pass||||||found_words = [x for x in [word_range(sentence2, i) for i in comorbidity_keywords] if not x is None]",
    "import httplib2|||from bs4 import BeautifulSoup, SoupStrainer||||||http = httplib2.Http()|||status, response = http.request('http://www.nytimes.com')||||||for link in BeautifulSoup(response, parse_only=SoupStrainer('a')):|||    if link.has_attr('href'):|||        print(link['href'])",
    "In [6]: import pickle|||In [14]: len(pickle.dumps(BigData(50)))|||Out[14]: 1052187||||||import math|||import numpy as np|||import time|||import sys|||import multiprocessing as mp|||import scipy.interpolate as interpolate||||||_tm=0|||def stopwatch(msg=''):|||    tm = time.time()|||    global _tm|||    if _tm==0: _tm = tm; return|||    print(\"%s: %.2f seconds\" % (msg, tm-_tm))|||    _tm = tm||||||class BigData:|||    def __init__(self, n):|||        z = np.random.uniform(size=n*n*n).reshape((n,n,n))|||        self.ff = []|||        for i in range(n):|||            f = interpolate.RectBivariateSpline(|||                np.arange(n), np.arange(n), z[i], kx=1, ky=1)|||            self.ff.append(f)|||        self.n = n||||||    def do_chunk(self, k, xi, yi):|||        n = self.n|||        s = np.sum(np.exp(self.ff[k].ev(xi, yi)))|||        sys.stderr.write(\".\")|||        return s||||||    def do_chunk_of_chunks(self, k_start, k_end, xi, yi):|||        s = sum(np.sum(np.exp(self.ff[k].ev(xi, yi)))|||                    for k in range(k_start, k_end))|||        sys.stderr.write(\".\")|||        return s||||||    def do_multi(self, numproc, xi, yi):|||        procs = []|||        pool = mp.Pool(numproc)|||        stopwatch('\nPool setup')|||        ks = list(map(int, np.linspace(0, self.n, numproc+1)))|||        for i in range(len(ks)-1):|||            k_start, k_end = ks[i:i+2]|||            p = pool.apply_async(_do_chunk_wrapper, (k_start, k_end, xi, yi))|||            procs.append(p)|||        stopwatch('Jobs queued (%d processes)' % numproc)|||        total = 0.0|||        for k, p in enumerate(procs):|||            total += np.sum(p.get(timeout=30)) # timeout allows ctrl-C interrupt|||            if k == 0: stopwatch(\"\nFirst get() done\")|||        print(total)|||        stopwatch('Jobs done')|||        pool.close()|||        pool.join()|||        return total||||||    def do_single(self, xi, yi):|||        total = 0.0|||        for k in range(self.n):|||            total += self.do_chunk(k, xi, yi)|||        stopwatch('\nAll in single process')|||        return total||||||def _do_chunk_wrapper(k_start, k_end, xi, yi): |||    return bd.do_chunk_of_chunks(k_start, k_end, xi, yi)        ||||||if __name__ == \"__main__\":|||    stopwatch()|||    n = 50|||    bd = BigData(n)|||    m = 1000*1000|||    xi, yi = np.random.uniform(0, n, size=m*2).reshape((2,m))|||    stopwatch('Initialized')|||    bd.do_multi(2, xi, yi)|||    bd.do_multi(3, xi, yi)|||    bd.do_single(xi, yi)",
    "with open('hr_data.csv', 'rU') as infile:|||    reader = list(csv.DictReader(infile, delimiter=',))",
    "window = tk.Tk()||||||[...]||||||window.after( 1000, myFunction, arg1 )||||||def checkPlaying( main_window ):|||    global current_track||||||    if ( mixer.music.get_busy() == False ):|||        # Playing has finished|||        print(\"Sound finished, playing next\" )|||        track_index, track_name = current_track    |||        current_track = ( track_index + 1, '' )     # Advance to next track|||        press( 'PLAY' )                             # Start playing||||||    # Queue the next call to this function|||    main_window.after( 250, checkPlaying, main_window )   # call again later||||||# Start the Music Playing Check|||app.after( 1000, checkPlaying, app )|||The press() function needs to be modified to remove the continuous loop, just playing the current sound-file:||||||mixer.init()                 |||current_track = ( 0, '' )    # track-index and name|||||||||def press(word):|||    global track_names|||    global current_track||||||    word = button_text.get()|||    if word == 'PLAY':|||        update_button_text('PLAY')|||        track_index, track_name = current_track|||        if ( track_index &gt;= len( track_names ) ):  # if out of music, re-start|||            track_index = 0||||||        # Start Playing the current track|||        name = track_names[ track_index ]|||        current_track = ( track_index, name )|||        mixer.music.set_volume(100)|||        mixer.music.load(f'C:/Users/user/Desktop/python/projects/etc/{name}.mp3')|||        mixer.music.play()|||        |||        # Update the GUI|||        window_2.delete(0, 'end')|||        window_2.configure(state='normal')|||        window_2.insert('end', name)||||||from tkinter import *|||import time, sys|||from pygame import mixer||||||mixer.init()||||||track_names = [ 'car-horn2', 'car-horn', 'cash-register', 'dog-bark', 'duck-quack', 'rain-falling', 'single-ding', 'turkey-gobble' ]|||current_track = ( 0, '' )|||||||||def press(word):|||    global track_names|||    global current_track||||||    word = button_text.get()|||    if word == 'PLAY':|||        update_button_text('PLAY')|||        track_index, track_name = current_track|||        if ( track_index &gt;= len( track_names ) ):  # if out of music, re-start|||            track_index = 0   ||||||        # Play the current track|||        name = track_names[ track_index ]|||        current_track = ( track_index, name )|||        mixer.music.set_volume(100)|||        #mixer.music.load(f'C:/Users/user/Desktop/python/projects/etc/{name}.mp3')|||        mixer.music.load(f'{name}.mp3')|||        mixer.music.play()||||||        window_2.delete(0, 'end')|||        window_2.configure(state='normal')|||        window_2.insert('end', name)||||||    if word == 'PAUSE':|||        update_button_text('PAUSE')|||        mixer.music.pause()|||        time.sleep(5)||||||    if word == 'STOP':|||        mixer.music.stop()|||        time.sleep(5)||||||    if word == 'NEXT':|||        pass||||||    if word == 'PREVIOUS':|||        pass|||||||||def checkPlaying( main_window ):|||    global track_names|||    global current_track||||||    result = False||||||    if ( mixer.music.get_busy() == True ):|||        # Still playing|||        result = True|||    else:|||        # Playing has finished|||        # TODO: Change button states, whatever|||        print(\"Sound finished, playing next\" )|||        track_index, track_name = current_track|||        current_track = ( track_index + 1, '' )|||        press( 'PLAY' )        # start next track|||        result = False||||||    # Queue the next call to this function|||    main_window.after( 250, checkPlaying, main_window )||||||    return result|||||||||def update_button_text(word):|||    if word == 'PLAY':|||        button_text.set('PAUSE')|||    elif word == 'PAUSE':|||        button_text.set('PLAY')|||||||||if __name__ == '__main__':|||    # create application window|||    app = Tk()||||||    # title|||    app.title(\"Music Players\")||||||    # geometry|||    app.geometry('383x121')||||||    # background color|||    app.configure(bg='orange')||||||    equation = StringVar()|||    window_1 = Label(app, textvariable=equation)|||    window_1.grid(columnspan=4, ipadx=100, ipady=10)|||    equation.set('music player')||||||    window_2 = Entry(app, width=30)|||    window_2.grid(columnspan=4, ipadx=100, ipady=10)|||    window_2.configure(state='disabled')||||||    window_2.grid_columnconfigure((0, 1, 2), uniform=\"equal\", weight=1)||||||    # Create buttons|||    button_text = StringVar()|||    button_text.set(\"PLAY\")|||    button1 = Button(app, textvariable=button_text, fg='yellow', bg='purple',|||                     command=lambda: press(button_text), height=2, width=1)|||    button1.grid(row=2, column=0, sticky=\"NSEW\")||||||    button2 = Button(app, text='STOP', fg='yellow', bg='purple',|||                     command=lambda: press('STOP'), height=2, width=1)|||    button2.grid(row=2, column=1, sticky=\"NSEW\")||||||    button3 = Button(app, text='NEXT', fg='yellow', bg='purple',|||                     command=lambda: press('NEXT'), height=2, width=1)|||    button3.grid(row=2, column=2, sticky=\"NSEW\")||||||    button4 = Button(app, text='PREVIOUS', fg='yellow', bg='purple',|||                     command=lambda: press('PREVIOUS'), height=2, width=1)|||    button4.grid(row=2, column=3, sticky=\"NSEW\")||||||    # Start the Music Playing Check|||    app.after( 1000, checkPlaying, app )||||||# start the GUI|||app.mainloop()",
    "import csv|||import requests |||from bs4 import BeautifulSoup|||for i in range(907):      # Number of pages plus one |||    url = \"http://www.pga.com/golf-courses/search?page={}&searchbox=Course+Name&searchbox_zip=ZIP&distance=50&price_range=0&course_type=both&has_events=0\".format(i)|||    r = requests.get(url)|||    soup = BeautifulSoup(r.content)||||||    # Your code for each individual page here ",
    "from your.application.file import app||||||import smtplib|||import logging|||from logging.handlers import RotatingFileHandler, SMTPHandler|||||||||# Provide a class to allow SSL (Not TLS) connection for mail handlers by overloading the emit() method|||class SSLSMTPHandler(SMTPHandler):|||    def emit(self, record):|||        \"\"\"|||        Emit a record.|||        \"\"\"|||        try:|||            port = self.mailport|||            if not port:|||                port = smtplib.SMTP_PORT|||            smtp = smtplib.SMTP_SSL(self.mailhost, port)|||            msg = self.format(record)|||            if self.username:|||                smtp.login(self.username, self.password)|||            smtp.sendmail(self.fromaddr, self.toaddrs, msg)|||            smtp.quit()|||        except (KeyboardInterrupt, SystemExit):|||            raise|||        except:|||            self.handleError(record)|||||||||# Create file handler for error/warning/info/debug logs|||file_handler = RotatingFileHandler('logs/app.log', maxBytes=1*1024*1024, backupCount=100)||||||# Apply format to the log messages|||formatter = logging.Formatter(\"[%(asctime)s] |  %(levelname)s | {%(pathname)s:%(lineno)d} | %(message)s\")|||file_handler.setFormatter(formatter)||||||# Set the level according to whether we're debugging or not|||if app.debug:|||    file_handler.setLevel(logging.DEBUG)|||else:|||    file_handler.setLevel(logging.WARN)||||||# Create equivalent mail handler|||mail_handler = SSLSMTPHandler(mailhost=(app.config['MAIL_SERVER'], app.config['MAIL_PORT']),|||                           fromaddr=app.config['MAIL_FROM_EMAIL'],|||                           toaddrs='my@emailaddress.com',|||                           subject='Your app died. Sad times...',|||                           credentials=(app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']))||||||# Set the email format|||mail_handler.setFormatter(logging.Formatter('''|||Message type:       %(levelname)s|||Location:           %(pathname)s:%(lineno)d|||Module:             %(module)s|||Function:           %(funcName)s|||Time:               %(asctime)s||||||Message:||||||%(message)s|||'''))||||||# Only email errors, not warnings|||mail_handler.setLevel(logging.ERROR)||||||# Register the handlers against all the loggers we have in play|||# This is done after app configuration and SQLAlchemy initialisation, |||# drop the sqlalchemy if not using - I thought a full example would be helpful.|||import logging|||from .utils.logs import mail_handler, file_handler|||loggers = [app.logger, logging.getLogger('sqlalchemy'), logging.getLogger('werkzeug')]|||for logger in loggers:|||    logger.addHandler(file_handler)|||    # Note - I added a boolean configuration parameter, MAIL_ON_ERROR, |||    # to allow direct control over whether to email on errors. |||    # You may wish to use 'if not app.debug' instead.|||    if app.config['MAIL_ON_ERROR']:|||        logger.addHandler(mail_handler)||||||from email.message import EmailMessage|||import email.utils|||class SSLSMTPHandler(SMTPHandler):||||||    def emit(self, record):|||        \"\"\"|||        Emit a record.|||        \"\"\"|||        try:|||            port = self.mailport|||            if not port:|||                port = smtplib.SMTP_PORT|||            smtp = smtplib.SMTP_SSL(self.mailhost, port)|||            msg = EmailMessage()|||            msg['From'] = self.fromaddr|||            msg['To'] = ','.join(self.toaddrs)|||            msg['Subject'] = self.getSubject(record)|||            msg['Date'] = email.utils.localtime()|||            msg.set_content(self.format(record))|||            if self.username:|||                smtp.login(self.username, self.password)|||            smtp.send_message(msg, self.fromaddr, self.toaddrs)|||            smtp.quit()|||        except (KeyboardInterrupt, SystemExit):|||            raise|||        except:|||            self.handleError(record)",
    "import urllib.request|||with urllib.request.urlopen('http://www.example.com/') as f:|||    html = f.read().decode('utf-8')||||||import urllib2|||response = urllib2.urlopen('http://www.example.com/')|||html = response.read()",
    "import base64||||||mylist = [['a', 'b'], ['c', 'd']]||||||# Creating the csv formatted string|||csv_line_length = len(max(mylist,key=len))|||csv_string = ''|||for row in mylist:|||    temp_row = ['\"' + col + '\"' for col in row]|||    while len(temp_row) &lt; csv_line_length:|||      temp_row.append([])|||    csv_string += ','.join(temp_row) + '\n'|||# Encoding the string to base64|||encoded = base64.b64encode(csv_string.encode('utf-8'))",
    "from pathlib import Path|||start_dir = Path('.')|||excel_files  = start_dir.glob('*/*.xlsx')|||list_of_dfs = [(filename, pd.read_excel(filename, header=0, dayfirst=True)) for filename in excel_files]||||||for filename, df in list_of_dfs:|||    try:|||        datetimes = pd.to_datetime(df.columns)|||        df.columns = datetimes |||    except ValueError:|||        print('failed to parse column in %s' % filename",
    "import sys,os|||IPYNB_FILENAME = 'test_argv.ipynb'|||CONFIG_FILENAME = '.config_ipynb'||||||def main(argv):|||    with open(CONFIG_FILENAME,'w') as f:|||        f.write(' '.join(argv))|||    os.system('jupyter nbconvert --execute {:s} --to html'.format(IPYNB_FILENAME))|||    return None||||||if __name__ == '__main__':|||    main(sys.argv)|||The notebook contains:||||||import sys,os,argparse|||from IPython.display import HTML|||CONFIG_FILE = '.config_ipynb'|||if os.path.isfile(CONFIG_FILE):|||    with open(CONFIG_FILE) as f:|||        sys.argv = f.read().split()|||else:|||    sys.argv = ['test_args.py', 'input_file', '--int_param', '12']||||||parser = argparse.ArgumentParser()|||parser.add_argument(\"input_file\",help=\"Input image, directory, or npy.\")|||parser.add_argument(\"--int_param\", type=int, default=4, help=\"an optional integer parameter.\")|||args = parser.parse_args()|||p = args.int_param|||print(args.input_file,p)"
];

table_data_IDE=[
    "None",
    "airports_in_asia",
    "if seq1 None",
    "if i None",
    "if None",
    "if num1 None",
    "mysock None format( )",
    "None connector format( )",
    "format( None )",
    "None format( )",
    "startswith( format( ))",
    "None format( )",
    "n_lines None",
    "None",
    "None",
    "format( None )",
    "subplot( format )",
    "None format( None )",
    "name None",
    "yield None",
    "only_name",
    "return None format( )",
    "return format(get_csv_column_average(filename))",
    "pygame",
    "except None",
    "file None file None",
    "r format( None )",
    "base64_img_bytes None format( )",
    "for name in f_names False",
    "diz format( i )"
];

table_data_T5=[
    "path = targetStream . Path",
    "airports_in_asia . append ( line [ 11 ] [    : ] )",
    "matrix [ y , 1 ] = y",
    "if i in centers :",
    "for i in range ( len ( words ) ) :",
    "return num1",
    "return data",
    "cursor . execute ( \"CREATE DATA  ASE %s\" % DATA  ASE )",
    "title = title . replace ( ' ' , ' ' )",
    "ftp . login ( ftp . user ( ) , ftp . password ( ) )",
    "split ( ) [ 1 ] )",
    "i",
    "row + 1 ) :",
    "in numbers :",
    "csv_in_file :",
    "info",
    "plt . title ( '  lack' )",
    "state = 1  ",
    "read ( ) )",
    "replace ( \" \" , \"\" ) )",
    "if image . startswith ( 'src=\"' ) :|||only_name . append ( image [    : ] )",
    "raise   alueError ( 'The length of observations is already part of the '|||'the length of the sample.' )",
    "count += 1|||if column_name in line :|||   sum += float ( line [ column_name ] )",
    "play_list . append ( item )",
    "yt_link = article . find ( 'a' , class_ = 'video_link' ) . p . text",
    "server . ehlo ( )|||server . starttls ( )|||recv_message = server . ehlo ( )|||recv_message = server . ehlo ( )|||recv_message = server . login ( password , recv_message )",
    "tag . set_text ( tag . text )",
    "file_to_save . write ( base  _img_bytes )",
    "if name . endswith ( '.py' ) :|||   print ( os . path . join ( root , name ) )",
    "file . write ( text )"
];

table_data_natural2code = [
    "urllib.request.URLopener()",
    "array[1].astype(float)",
    "np.linspace(1, 3, color='green', marker='o')",
    "all(x > 2 for x in range(100))",
    "print(sorted(set(text), key=len))",
    "exec(num3)",
    "self.root.destroy()",
    "driver.get('CREATE DATABASE experiment')",
    "open('ICSE.pdf', 'rb').savefig('ICSE.pdf')",
    "requests.get('', verify=False)",
    "print(' '.join([str(i.strip()) for i in line.split()]))",
    "\"\"\"\"\"\".upper()",
    "N.delete(1, 2)",
    "max(range(1, 10), key=lambda x: x[1])",
    "print(list(itertools.chain(*line)))",
    "\"\"\"str_2\"\"\".encode('utf8').decode('utf8')",
    "plt.plot(list(range(100)))",
    "np.linspace(100, 100)",
    "plt.savefig('', dpi=1000)",
    "sys.exit(0)",
    "print(img.show())|||print(input[-1])",
    "len(1 for i in range(100) if i > 5)|||print(_observations[0])",
    "sum(float(x) for x in str(sum))|||max(abs(x) for x in range(100))",
    "datetime.strptime('', '').strftime('%Y-%m-%d')|||locale.setlocale(locale.LC_ALL, 'en_US')|||re.sub('(?<=.)(?=.)', '', string)",
    "element.update({'path': 'src'})|||print([x.replace('/', '') for x in x.split('/')])|||element.split('?')|||c.update({'v=': 'str_2'})",
    "requests.get('', auth=('', 'method'))|||result.result(**data)",
    "self.request.get('href', 'result')|||urllib.request.urlretrieve('articles', 'ignore')|||print('\n'.join([]))",
    "base64.urlsafe_b64decode(text.read(0))|||file_to_save.strftime('%m/%d/%Y')",
    "print([i for i in range(10)])|||np.where(-np.array(np.arange(np.arange(empty))))|||print('Hello {user[name]}'.format(key=lambda x: (x[1], x[2])))|||print(' %s, ' % (1, 2, 3))",
    "file.write(open('Recognized Speech:', 'rb').read())|||file.write(open('\n', 'rb').read())|||f.write(open('file', 'r').read())|||print(' %s, ' % (1, 2, 3))"
]

var tasks = {};

tasks["description"] = table_data_description

tasks["masked_code"] = table_data_masked_code

tasks["retrieval"]=table_data_retrieval
tasks["IDE"]=table_data_IDE
tasks["T5"]=table_data_T5
tasks["N2C"]=table_data_natural2code
tasks["validation"]=table_data_validation